{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45cdfc05",
   "metadata": {},
   "source": [
    "# myDataLoading: Loading DIM Tables Data into PostgreSQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a036f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import psycopg2\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Fetch database connection details from environment variables\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        # Define S3 bucket and file paths for each table\n",
    "        bucket_name = 'datalakepartition2'\n",
    "        keys = {\n",
    "            'dimplayer': 'dataframes/dimplayer.csv',\n",
    "            'dimteams': 'dataframes/dimteams.csv',\n",
    "            'dimvenue': 'dataframes/dimvenue.csv',\n",
    "            'dimmatches': 'dataframes/dimmatches.csv'\n",
    "        }\n",
    "\n",
    "        # Process and insert each CSV file into its respective table\n",
    "        for table_name, s3_key in keys.items():\n",
    "            df = load_csv_from_s3(bucket_name, s3_key)\n",
    "            logger.info(f\"Loaded data for '{table_name}' from S3 key '{s3_key}':\\n{df.head()}\")\n",
    "\n",
    "            # Process the DataFrame according to the table\n",
    "            df = process_dataframe(df, table_name)\n",
    "\n",
    "            # Insert the DataFrame into the SQL table\n",
    "            insert_dataframe_to_sql(df, table_name)\n",
    "        \n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': 'Data processing and SQL insertion for all tables completed successfully.'\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data: {str(e)}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': f\"Error processing data: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def load_csv_from_s3(bucket_name, s3_key):\n",
    "    try:\n",
    "        # Load a CSV file from S3 and return as a DataFrame\n",
    "        logger.info(f\"Loading file '{s3_key}' from bucket '{bucket_name}'\")\n",
    "        csv_obj = s3_client.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "        body = csv_obj['Body']\n",
    "        csv_string = body.read().decode('utf-8')\n",
    "        return pd.read_csv(io.StringIO(csv_string))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading CSV from S3: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_dataframe(df, table_name):\n",
    "    # Replace empty strings with NaN\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    logger.info(f\"Initial data statistics for '{table_name}':\\n{df.describe(include='all')}\")\n",
    "\n",
    "    # Define primary keys and data types based on table names\n",
    "    primary_keys = {\n",
    "        'dimplayer': 'player_id',\n",
    "        'dimteams': 'team_id',\n",
    "        'dimvenue': 'venue_id',\n",
    "        'dimmatches': 'fixture_id'\n",
    "    }\n",
    "\n",
    "    primary_key = primary_keys[table_name]\n",
    "    \n",
    "    if primary_key not in df.columns:\n",
    "        logger.error(f\"Primary key column '{primary_key}' not found in table '{table_name}'\")\n",
    "        raise KeyError(f\"Primary key column '{primary_key}' not found in table '{table_name}'\")\n",
    "    \n",
    "    if table_name in ['dimplayer', 'dimteams', 'dimvenue']:\n",
    "        df[primary_key] = validate_and_convert_int(df[primary_key], fill_value=0)\n",
    "        df = df.drop_duplicates(subset=primary_key, keep='first').dropna()\n",
    "\n",
    "    elif table_name == 'dimmatches':\n",
    "        df['fixture_id'] = validate_and_convert_int(df['fixture_id'], fill_value=0)\n",
    "        df['home_team_id'] = validate_and_convert_int(df['home_team_id'], fill_value=0)\n",
    "        df['away_team_id'] = validate_and_convert_int(df['away_team_id'], fill_value=0)\n",
    "        df['home_goals'] = validate_and_convert_float(df['home_goals'], fill_value=0.0)\n",
    "        df['away_goals'] = validate_and_convert_float(df['away_goals'], fill_value=0.0)\n",
    "        df['home_winner'] = validate_and_convert_float(df['home_winner'], fill_value=0.0)\n",
    "        df['away_winner'] = validate_and_convert_float(df['away_winner'], fill_value=0.0)\n",
    "        df['venue_id'] = validate_and_convert_int(df['venue_id'], fill_value=0)  # Add the venue_id conversion\n",
    "\n",
    "        # Convert timestamp column to datetime\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s', errors='coerce')\n",
    "        df = df.drop_duplicates(subset='fixture_id', keep='first').dropna()\n",
    "\n",
    "    logger.info(f\"Cleaned data statistics for '{table_name}':\\n{df.describe(include='all')}\")\n",
    "    logger.info(f\"Cleaned DataFrame for '{table_name}':\\n{df}\")\n",
    "    return df\n",
    "\n",
    "def validate_and_convert_int(series, fill_value=0):\n",
    "    \"\"\"\n",
    "    Validate and convert a pandas series to integers.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Validating and converting integer series. Initial data:\\n{series.head()}\")\n",
    "    series = pd.to_numeric(series, errors='coerce').fillna(fill_value).astype(int)\n",
    "    return series\n",
    "\n",
    "def validate_and_convert_float(series, fill_value=0.0):\n",
    "    \"\"\"\n",
    "    Validate and convert a pandas series to floats.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Validating and converting float series. Initial data:\\n{series.head()}\")\n",
    "    series = pd.to_numeric(series, errors='coerce').fillna(fill_value).astype(float)\n",
    "    return series\n",
    "\n",
    "def insert_dataframe_to_sql(df, table_name):\n",
    "    try:\n",
    "        # Establish a connection to the PostgreSQL database\n",
    "        conn = psycopg2.connect(\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT,\n",
    "            database=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Prepare the SQL insert statements for each table\n",
    "        insert_queries = {\n",
    "            'dimplayer': \"INSERT INTO dimplayer (player_id, player_name) VALUES (%s, %s) ON CONFLICT (player_id) DO NOTHING;\",\n",
    "            'dimteams': \"INSERT INTO dimteams (team_id, team_name) VALUES (%s, %s) ON CONFLICT (team_id) DO NOTHING;\",\n",
    "            'dimvenue': \"INSERT INTO dimvenue (venue_id, venue_name, venue_city) VALUES (%s, %s, %s) ON CONFLICT (venue_id) DO NOTHING;\",\n",
    "            'dimmatches': \"\"\"\n",
    "                INSERT INTO dimmatches (fixture_id, referee, date, time, timestamp, status_short,\n",
    "                                        home_team_id, home_team_name, home_winner, home_goals,\n",
    "                                        away_team_id, away_team_name, away_winner, away_goals, venue_id)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (fixture_id) DO NOTHING;\n",
    "            \"\"\"\n",
    "        }\n",
    "\n",
    "        insert_query = insert_queries[table_name]\n",
    "        \n",
    "        # Iterate over each row in the DataFrame and insert into the table\n",
    "        for row in df.itertuples(index=False):\n",
    "            cursor.execute(insert_query, row)\n",
    "        \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        logger.info(f\"All rows inserted into '{table_name}' successfully.\")\n",
    "        \n",
    "        # Close the cursor and connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inserting data into SQL table '{table_name}': {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f3c302",
   "metadata": {},
   "source": [
    "# myDataLoading2: Storing Stats Data into a easily loadable DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab29312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import logging\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Define S3 bucket and file suffixes\n",
    "    bucket_name = 'datalakepartition2'\n",
    "    suffix_statistics_fixture_all = 'statistics_fixtures_all.csv'\n",
    "    suffix_statistics_players_all = 'statistics_players_all.csv'\n",
    "    destination_folder = 'dataframes/'\n",
    "\n",
    "    try:\n",
    "        # Load and process files with the specified suffixes\n",
    "        stats_fixture_all_df = load_and_merge_files(bucket_name, suffix_statistics_fixture_all)\n",
    "        stats_players_all_df = load_and_merge_files(bucket_name, suffix_statistics_players_all)\n",
    "\n",
    "        # Log DataFrame columns for debugging\n",
    "        logger.info(f\"Columns in stats_fixture_all_df: {stats_fixture_all_df.columns.tolist()}\")\n",
    "        logger.info(f\"Columns in stats_players_all_df: {stats_players_all_df.columns.tolist()}\")\n",
    "\n",
    "        # Process each DataFrame\n",
    "        factteamsstats_df = process_factteamsstats(stats_fixture_all_df)\n",
    "        factplayerstats_df = process_factplayerstats(stats_players_all_df)\n",
    "\n",
    "        # Remove duplicates from DataFrames\n",
    "        factteamsstats_df = remove_duplicates(factteamsstats_df, subset=['fixture_id', 'team_id'])\n",
    "        factplayerstats_df = remove_duplicates(factplayerstats_df, subset=['fixture_id', 'team_id', 'player_id'])\n",
    "\n",
    "        # Save processed DataFrames to S3\n",
    "        save_df_to_s3(bucket_name, factteamsstats_df, f'{destination_folder}factteamsstats.csv')\n",
    "        save_df_to_s3(bucket_name, factplayerstats_df, f'{destination_folder}factplayerstats.csv')\n",
    "\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': 'Data processing and CSV generation completed successfully.'\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data: {str(e)}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': f\"Error processing data: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def load_and_merge_files(bucket_name, suffix):\n",
    "    # List all files with the specified suffix\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "    files = [item['Key'] for item in response.get('Contents', []) if item['Key'].endswith(suffix)]\n",
    "    \n",
    "    logger.info(f\"Files found with suffix '{suffix}': {files}\")\n",
    "    \n",
    "    if not files:\n",
    "        logger.warning(f\"No files found with suffix '{suffix}' in bucket '{bucket_name}'.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no files found\n",
    "    \n",
    "    # Load and concatenate all matching files\n",
    "    dataframes = [load_csv_from_s3(bucket_name, file) for file in files]\n",
    "    if not dataframes:\n",
    "        logger.warning(f\"No data loaded for suffix '{suffix}'. Returning an empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "def load_csv_from_s3(bucket_name, file_key):\n",
    "    # Load a CSV file from S3 and return as a DataFrame\n",
    "    logger.info(f\"Loading file '{file_key}' from bucket '{bucket_name}'\")\n",
    "    csv_obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    df = pd.read_csv(io.StringIO(csv_string), dtype=str)\n",
    "\n",
    "    # Log the columns of the loaded DataFrame\n",
    "    logger.info(f\"Loaded DataFrame columns from file '{file_key}': {df.columns.tolist()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_df_to_s3(bucket_name, df, file_key):\n",
    "    # Convert DataFrame to CSV and upload to S3\n",
    "    if df.empty:\n",
    "        logger.warning(f\"DataFrame is empty. Skipping upload for '{file_key}'.\")\n",
    "        return\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=file_key, Body=csv_buffer.getvalue())\n",
    "    logger.info(f\"DataFrame saved to '{file_key}' in bucket '{bucket_name}'\")\n",
    "\n",
    "def remove_duplicates(df, subset=None):\n",
    "    # Remove duplicates from the DataFrame based on the given subset\n",
    "    return df.drop_duplicates(subset=subset)\n",
    "\n",
    "def process_factteamsstats(df):\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    columns_mapping = {\n",
    "        'fixtureID': 'fixture_id',\n",
    "        'team_Id': 'team_id',\n",
    "        'Shots on Goal': 'shots_on_goal',\n",
    "        'Shots off Goal': 'shots_off_goal',\n",
    "        'Blocked Shots': 'blocked_shots',\n",
    "        'Shots insidebox': 'shots_insidebox',\n",
    "        'Shots outsidebox': 'shots_outsidebox',\n",
    "        'Fouls': 'fouls',\n",
    "        'Corner Kicks': 'corner_kicks',\n",
    "        'Offsides': 'offsides',\n",
    "        'Ball Possession': 'ball_possession',\n",
    "        'Yellow Cards': 'yellow_cards',\n",
    "        'Red Cards': 'red_cards',\n",
    "        'Goalkeeper Saves': 'goalkeeper_saves',\n",
    "        'Total passes': 'total_passes',\n",
    "        'Passes accurate': 'passes_accurate',\n",
    "        'Passes %': 'passes_percentage'\n",
    "    }\n",
    "\n",
    "    # Check if all required columns are present\n",
    "    missing_columns = [col for col in columns_mapping.keys() if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        logger.error(f\"Missing columns in stats_fixture_all_df: {missing_columns}\")\n",
    "        raise ValueError(f\"Missing columns in stats_fixture_all_df: {missing_columns}\")\n",
    "\n",
    "    # Rename columns according to the mapping\n",
    "    df = df[list(columns_mapping.keys())].rename(columns=columns_mapping)\n",
    "\n",
    "    # Replace 'nan' string with actual NaN\n",
    "    df.replace('nan', pd.NA, inplace=True)\n",
    "\n",
    "    # Convert columns to appropriate types\n",
    "    df['fixture_id'] = df['fixture_id'].astype(int)\n",
    "    df['team_id'] = df['team_id'].astype(int)\n",
    "    for col in ['shots_on_goal', 'shots_off_goal', 'blocked_shots', 'shots_insidebox', 'shots_outsidebox',\n",
    "                'fouls', 'corner_kicks', 'offsides', 'yellow_cards', 'red_cards', 'goalkeeper_saves', 'total_passes', 'passes_accurate']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "    for col in ['ball_possession', 'passes_percentage']:\n",
    "        df[col] = df[col].str.replace('%', '').astype(float) / 100.0\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_factplayerstats(df):\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Updated columns mapping to match the provided CSV file\n",
    "    columns_mapping = {\n",
    "        'fixtureID': 'fixture_id',\n",
    "        'team_Id': 'team_id',\n",
    "        'player_id': 'player_id',\n",
    "        'minutes': 'player_statistics_games_minutes',\n",
    "        'number': 'player_statistics_games_number',\n",
    "        'rating': 'games_rating',\n",
    "        'offsides': 'games_offside',\n",
    "        'goals_total': 'goals_total',\n",
    "        'goals_conceded': 'goals_conceded',\n",
    "        'goals_assists': 'goals_assists',\n",
    "        'passes_total': 'passes_total',\n",
    "        'passes_accuracy': 'passes_accuracy',\n",
    "        'tackles_total': 'tackles_total',\n",
    "        'duels_total': 'duels_total',\n",
    "        'fouls_committed': 'fouls_committed'\n",
    "    }\n",
    "\n",
    "    # Check if all required columns are present\n",
    "    missing_columns = [col for col in columns_mapping.keys() if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        logger.error(f\"Missing columns in stats_players_all_df: {missing_columns}\")\n",
    "        raise ValueError(f\"Missing columns in stats_players_all_df: {missing_columns}\")\n",
    "\n",
    "    # Select and rename columns according to the mapping\n",
    "    df = df[list(columns_mapping.keys())].rename(columns=columns_mapping)\n",
    "\n",
    "    # Replace 'nan' string with actual NaN\n",
    "    df.replace('nan', pd.NA, inplace=True)\n",
    "\n",
    "    # Convert columns to appropriate types\n",
    "    try:\n",
    "        df['fixture_id'] = pd.to_numeric(df['fixture_id'], errors='coerce').fillna(0).astype(int)\n",
    "        df['team_id'] = pd.to_numeric(df['team_id'], errors='coerce').fillna(0).astype(int)\n",
    "        df['player_id'] = pd.to_numeric(df['player_id'], errors='coerce').fillna(0).astype(int)\n",
    "        \n",
    "        # Convert numeric columns, handling NaNs appropriately\n",
    "        numeric_cols = ['player_statistics_games_minutes', 'player_statistics_games_number', 'games_rating', 'games_offside', \n",
    "                        'goals_total', 'goals_conceded', 'goals_assists', 'passes_total', 'passes_accuracy', 'tackles_total', \n",
    "                        'duels_total', 'fouls_committed']\n",
    "        for col in numeric_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting factplayerstats columns: {str(e)}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1414cd94",
   "metadata": {},
   "source": [
    "# myDataLoading3: Loading Stats Data into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e3903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Database connection details from environment variables\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_PORT = int(os.getenv('DB_PORT', 5432))  # Default to 5432 if DB_PORT is not set\n",
    "\n",
    "# Hard-coded database connection details. - TESTDATABASE\n",
    "# DB_HOST = 'database-3.cto9dqorm6ji.us-east-1.rds.amazonaws.com'\n",
    "# DB_NAME = 'databasetest'\n",
    "# DB_USER = 'postgres'\n",
    "# DB_PASSWORD = 'HSLU2024,rai!'\n",
    "# DB_PORT = 5432\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Define S3 bucket and file paths\n",
    "    bucket_name = 'datalakepartition2'\n",
    "    file_key_teams = 'dataframes/factteamsstats.csv'\n",
    "    file_key_players = 'dataframes/factplayerstats.csv'\n",
    "\n",
    "    connection = None\n",
    "    try:\n",
    "        # Load the CSV files from S3\n",
    "        teams_df = load_csv_from_s3(bucket_name, file_key_teams)\n",
    "        players_df = load_csv_from_s3(bucket_name, file_key_players)\n",
    "\n",
    "        # Log DataFrame columns for debugging\n",
    "        logger.info(f\"Columns in teams_df: {teams_df.columns.tolist()}\")\n",
    "        logger.info(f\"Columns in players_df: {players_df.columns.tolist()}\")\n",
    "\n",
    "        # Convert data types for teams DataFrame\n",
    "        teams_df = convert_teams_df_types(teams_df)\n",
    "        players_df = convert_players_df_types(players_df)\n",
    "\n",
    "        # Connect to the PostgreSQL database\n",
    "        connection = connect_to_db()\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Load data into SQL database\n",
    "        load_data_to_sql(teams_df, cursor, 'fact_teams_stats')\n",
    "        load_data_to_sql(players_df, cursor, 'fact_player_stats')\n",
    "\n",
    "        # Commit the transaction\n",
    "        connection.commit()\n",
    "\n",
    "        logger.info(\"Data loaded into the SQL database successfully.\")\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': 'Data loaded into the SQL database successfully.'\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data into the SQL database: {str(e)}\")\n",
    "        if connection:\n",
    "            connection.rollback()  # Rollback in case of error\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': f\"Error loading data into the SQL database: {str(e)}\"\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "def load_csv_from_s3(bucket_name, file_key):\n",
    "    \"\"\"Load a CSV file from S3 and return as a DataFrame.\"\"\"\n",
    "    logger.info(f\"Loading file '{file_key}' from bucket '{bucket_name}'\")\n",
    "    csv_obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    df = pd.read_csv(io.StringIO(csv_string), dtype=str)  # Read as strings to handle all types\n",
    "    return df\n",
    "\n",
    "def connect_to_db():\n",
    "    \"\"\"Connect to the PostgreSQL database using hard-coded details.\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT,\n",
    "            database=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD\n",
    "        )\n",
    "        logger.info(\"Successfully connected to the database\")\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error connecting to the database: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_data_to_sql(df, cursor, table_name):\n",
    "    \"\"\"Load data from DataFrame into SQL table.\"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(f\"The DataFrame for table '{table_name}' is empty. Skipping load.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare SQL query for inserting data\n",
    "    columns = df.columns.tolist()\n",
    "    columns_str = ', '.join(columns)\n",
    "    values_str = ', '.join(['%s'] * len(columns))\n",
    "\n",
    "    insert_query = f\"INSERT INTO {table_name} ({columns_str}) VALUES ({values_str})\"\n",
    "\n",
    "    # Log the SQL query for debugging\n",
    "    logger.info(f\"Insert query for table '{table_name}': {insert_query}\")\n",
    "\n",
    "    # Convert DataFrame rows to list of tuples\n",
    "    data_tuples = [tuple(x) for x in df.to_numpy()]\n",
    "\n",
    "    # Execute the SQL query using batch insert for efficiency\n",
    "    try:\n",
    "        psycopg2.extras.execute_batch(cursor, insert_query, data_tuples)\n",
    "        logger.info(f\"Loaded data into table '{table_name}' successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error executing SQL insert for table '{table_name}': {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def convert_teams_df_types(df):\n",
    "    \"\"\"Convert data types for the teams DataFrame.\"\"\"\n",
    "    try:\n",
    "        df['fixture_id'] = df['fixture_id'].astype(int)\n",
    "        df['team_id'] = df['team_id'].astype(int)\n",
    "        df['shots_on_goal'] = df['shots_on_goal'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['shots_off_goal'] = df['shots_off_goal'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['blocked_shots'] = df['blocked_shots'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['shots_insidebox'] = df['shots_insidebox'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['shots_outsidebox'] = df['shots_outsidebox'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['fouls'] = df['fouls'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['corner_kicks'] = df['corner_kicks'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['offsides'] = df['offsides'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['ball_possession'] = df['ball_possession'].astype(float)\n",
    "        df['yellow_cards'] = df['yellow_cards'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['red_cards'] = df['red_cards'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['goalkeeper_saves'] = df['goalkeeper_saves'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['total_passes'] = df['total_passes'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['passes_accurate'] = df['passes_accurate'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['passes_percentage'] = df['passes_percentage'].astype(float)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting data types for teams DataFrame: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_players_df_types(df):\n",
    "    \"\"\"Convert data types for the players DataFrame.\"\"\"\n",
    "    try:\n",
    "        df['fixture_id'] = df['fixture_id'].astype(int)\n",
    "        df['team_id'] = df['team_id'].astype(int)\n",
    "        df['player_id'] = df['player_id'].astype(int)\n",
    "        df['player_statistics_games_minutes'] = df['player_statistics_games_minutes'].astype(float)\n",
    "        df['player_statistics_games_number'] = df['player_statistics_games_number'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['games_rating'] = df['games_rating'].astype(float)\n",
    "        df['games_offside'] = df['games_offside'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['goals_total'] = df['goals_total'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['goals_conceded'] = df['goals_conceded'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['goals_assists'] = df['goals_assists'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['passes_total'] = df['passes_total'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['passes_accuracy'] = df['passes_accuracy'].astype(float)\n",
    "        df['tackles_total'] = df['tackles_total'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['duels_total'] = df['duels_total'].apply(lambda x: int(float(x) if x else 0))\n",
    "        df['fouls_committed'] = df['fouls_committed'].apply(lambda x: int(float(x) if x else 0))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting data types for players DataFrame: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a57e36",
   "metadata": {},
   "source": [
    "# myDataLoading4: Storing DIM TABLES Data into DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f05d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import logging\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Define S3 bucket and file suffixes\n",
    "    bucket_name = 'datalakepartition2'\n",
    "    suffix_fixtures = 'fixtures.csv'\n",
    "    suffix_statistics_fixture_all = 'statistics_fixtures_all.csv'\n",
    "    suffix_statistics_players_all = 'statistics_players_all.csv'\n",
    "    destination_folder = 'dataframes/'\n",
    "\n",
    "    try:\n",
    "        # Load and process files with the specified suffixes\n",
    "        fixtures_df = load_and_merge_files(bucket_name, suffix_fixtures)\n",
    "        stats_fixture_all_df = load_and_merge_files(bucket_name, suffix_statistics_fixture_all)\n",
    "        stats_players_all_df = load_and_merge_files(bucket_name, suffix_statistics_players_all)\n",
    "\n",
    "        # Process each DataFrame\n",
    "        dimmatches_df = process_dimmatches(fixtures_df)\n",
    "        dimvenue_df = process_dimvenue(fixtures_df)\n",
    "        dimteams_df = process_dimteams(stats_fixture_all_df)\n",
    "        dimplayer_df = process_dimplayer(stats_players_all_df)\n",
    "\n",
    "        # Remove duplicates from DataFrames\n",
    "        dimmatches_df = remove_duplicates(dimmatches_df)\n",
    "        dimvenue_df = remove_duplicates(dimvenue_df)\n",
    "        dimteams_df = remove_duplicates(dimteams_df)\n",
    "        dimplayer_df = remove_duplicates(dimplayer_df)\n",
    "\n",
    "        # Save processed DataFrames to S3\n",
    "        save_df_to_s3(bucket_name, dimmatches_df, f'{destination_folder}dimmatches.csv')\n",
    "        save_df_to_s3(bucket_name, dimvenue_df, f'{destination_folder}dimvenue.csv')\n",
    "        save_df_to_s3(bucket_name, dimteams_df, f'{destination_folder}dimteams.csv')\n",
    "        save_df_to_s3(bucket_name, dimplayer_df, f'{destination_folder}dimplayer.csv')\n",
    "\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': 'Data processing and upload completed successfully.'\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data: {str(e)}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': f\"Error processing data: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def load_and_merge_files(bucket_name, suffix):\n",
    "    # List all files with the specified suffix\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "    files = [item['Key'] for item in response.get('Contents', []) if item['Key'].endswith(suffix)]\n",
    "    \n",
    "    logger.info(f\"Files found with suffix '{suffix}': {files}\")\n",
    "    \n",
    "    if not files:\n",
    "        logger.warning(f\"No files found with suffix '{suffix}' in bucket '{bucket_name}'.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no files found\n",
    "    \n",
    "    # Load and concatenate all matching files\n",
    "    dataframes = [load_csv_from_s3(bucket_name, file) for file in files]\n",
    "    if not dataframes:\n",
    "        logger.warning(f\"No data loaded for suffix '{suffix}'. Returning an empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "def load_csv_from_s3(bucket_name, file_key):\n",
    "    # Load a CSV file from S3 and return as a DataFrame\n",
    "    logger.info(f\"Loading file '{file_key}' from bucket '{bucket_name}'\")\n",
    "    csv_obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    return pd.read_csv(io.StringIO(csv_string))\n",
    "\n",
    "def save_df_to_s3(bucket_name, df, file_key):\n",
    "    # Convert DataFrame to CSV and upload to S3\n",
    "    if df.empty:\n",
    "        logger.warning(f\"DataFrame is empty. Skipping upload for '{file_key}'.\")\n",
    "        return\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=file_key, Body=csv_buffer.getvalue())\n",
    "    logger.info(f\"DataFrame saved to '{file_key}' in bucket '{bucket_name}'\")\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    # Remove duplicates from the DataFrame\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def process_dimmatches(df):\n",
    "    # Transform fixtures DataFrame into dimmatches DataFrame including venue_id\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    return df[['fixture_id', 'referee', 'date', 'time', 'timestamp', 'status_short',\n",
    "               'home_team_id', 'home_team_name', 'home_winner', 'home_goals',\n",
    "               'away_team_id', 'away_team_name', 'away_winner', 'away_goals',\n",
    "               'venue_id']]\n",
    "\n",
    "def process_dimvenue(df):\n",
    "    # Transform fixtures DataFrame into dimvenue DataFrame\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    return df[['venue_id', 'venue_name', 'venue_city']]\n",
    "\n",
    "def process_dimteams(df):\n",
    "    # Transform statistics_fixture_all DataFrame into dimteams DataFrame\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    return df[['team_Id', 'team_name']].rename(columns={'team_Id': 'team_id'})\n",
    "\n",
    "def process_dimplayer(df):\n",
    "    # Transform statistics_players_all DataFrame into dimplayer DataFrame\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    return df[['player_id', 'player_name']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ea5298",
   "metadata": {},
   "source": [
    "# myDataLoading5: Loading Forecast16d and historic weather data into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import logging\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extras import execute_batch\n",
    "import os\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Database connection details from environment variables\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_PORT = int(os.getenv('DB_PORT', 5432))\n",
    "\n",
    "# PostgreSQL connection details - TESTDATABASE\n",
    "# DB_HOST = 'database-3.cto9dqorm6ji.us-east-1.rds.amazonaws.com'\n",
    "# DB_NAME = 'databasetest'\n",
    "# DB_USER = 'postgres'\n",
    "# DB_PASSWORD = 'HSLU2024,rai!'\n",
    "# DB_PORT = 5432\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Define S3 bucket and file suffixes\n",
    "    bucket_name = 'datalakepartition2'\n",
    "    file_mappings = {\n",
    "        'predictions16days': 'factweatherFC16d',\n",
    "        'historic_weather': 'factweather'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Load and process each type of file\n",
    "        for source_file, dest_table in file_mappings.items():\n",
    "            suffix = f'{source_file}.csv'\n",
    "            source_df = load_and_merge_files(bucket_name, suffix)\n",
    "            if not source_df.empty:\n",
    "                processed_df = process_dataframe(source_file, source_df)\n",
    "                \n",
    "                # Remove duplicates and empty values\n",
    "                processed_df = processed_df.drop_duplicates().dropna()\n",
    "\n",
    "                # Save DataFrame to PostgreSQL\n",
    "                save_df_to_postgres(processed_df, dest_table)\n",
    "            else:\n",
    "                logger.warning(f\"No data processed for file type '{source_file}'\")\n",
    "\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': 'Data processing and upload completed successfully.'\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data: {str(e)}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': f\"Error processing data: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def load_and_merge_files(bucket_name, suffix):\n",
    "    # Use pagination to list all files with the specified suffix\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name)\n",
    "\n",
    "    files = []\n",
    "    for page in page_iterator:\n",
    "        files.extend([item['Key'] for item in page.get('Contents', []) if item['Key'].endswith(suffix)])\n",
    "    \n",
    "    logger.info(f\"Files found with suffix '{suffix}': {files}\")\n",
    "    \n",
    "    if not files:\n",
    "        logger.warning(f\"No files found with suffix '{suffix}' in bucket '{bucket_name}'.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no files found\n",
    "    \n",
    "    # Load and concatenate all matching files\n",
    "    dataframes = [load_csv_from_s3(bucket_name, file) for file in files]\n",
    "    if not dataframes:\n",
    "        logger.warning(f\"No data loaded for suffix '{suffix}'. Returning an empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "def load_csv_from_s3(bucket_name, file_key):\n",
    "    # Load a CSV file from S3 and return as a DataFrame\n",
    "    logger.info(f\"Loading file '{file_key}' from bucket '{bucket_name}'\")\n",
    "    csv_obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    return pd.read_csv(io.StringIO(csv_string))\n",
    "\n",
    "def process_dataframe(source_file, df):\n",
    "    # Transform the source DataFrame into the required destination DataFrame structure\n",
    "    column_mappings = {\n",
    "        'predictions16days': [\n",
    "            ('city_id', None), \n",
    "            ('city_name', 'venue_city'),\n",
    "            ('coord_lon', 'coord_lon'),\n",
    "            ('coord_lat', 'coord_lat'),\n",
    "            ('country', 'country'),\n",
    "            ('population', 'population'),\n",
    "            ('timezone', 'timezone'),\n",
    "            ('date', 'date'),\n",
    "            ('time', 'time'),\n",
    "            ('temp_day', 'temperature'),\n",
    "            ('pressure', 'pressure'),\n",
    "            ('humidity', 'humidity'),\n",
    "            ('weather_description', 'weather_description'),\n",
    "            ('wind_speed', 'wind_speed'),\n",
    "            ('wind_deg', None),\n",
    "            ('rain', 'rain_volume')\n",
    "        ],\n",
    "        'historic_weather': [\n",
    "            ('city_id', 'venue_city'),\n",
    "            ('datetime', None),\n",
    "            ('date', 'date'),\n",
    "            ('time', 'time'),\n",
    "            ('temp', 'temperature'),\n",
    "            ('pressure', 'pressure'),\n",
    "            ('humidity', 'humidity'),\n",
    "            ('wind_speed', 'wind_speed'),\n",
    "            ('clouds', None),\n",
    "            ('weather_description', 'weather_description')\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    if source_file not in column_mappings:\n",
    "        logger.error(f\"No column mappings found for source file '{source_file}'\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    mappings = column_mappings[source_file]\n",
    "    transformed_df = pd.DataFrame()\n",
    "\n",
    "    for mapping in mappings:\n",
    "        if len(mapping) == 2:\n",
    "            source_col, dest_col = mapping\n",
    "        elif len(mapping) == 3:\n",
    "            source_col, dest_col, _ = mapping\n",
    "        else:\n",
    "            logger.error(f\"Invalid column mapping: {mapping}\")\n",
    "            continue\n",
    "\n",
    "        if dest_col:\n",
    "            transformed_df[dest_col] = df[source_col]\n",
    "    \n",
    "    return transformed_df\n",
    "\n",
    "def save_df_to_postgres(df, table_name):\n",
    "    # Connect to the PostgreSQL database and save the DataFrame\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT,\n",
    "            database=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Prepare the insert statement\n",
    "        columns = df.columns.tolist()\n",
    "        insert_query = sql.SQL(\"INSERT INTO {} ({}) VALUES ({})\").format(\n",
    "            sql.Identifier(table_name),\n",
    "            sql.SQL(', ').join(map(sql.Identifier, columns)),\n",
    "            sql.SQL(', ').join(sql.Placeholder() * len(columns))\n",
    "        )\n",
    "\n",
    "        # Convert DataFrame to list of tuples for batch insertion\n",
    "        data = df.to_records(index=False).tolist()\n",
    "\n",
    "        # Execute batch insert\n",
    "        execute_batch(cursor, insert_query, data)\n",
    "\n",
    "        # Commit the transaction\n",
    "        connection.commit()\n",
    "        logger.info(f\"Data saved to table '{table_name}' in PostgreSQL database\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving data to PostgreSQL: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d96b192",
   "metadata": {},
   "source": [
    "# myDataLoading6: Storing Forecast4days into DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import logging\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Define S3 bucket and file suffix for factweatherFC4d\n",
    "    bucket_name = 'datalakepartition2'\n",
    "    source_file = 'predictions4days'\n",
    "    dest_df = 'factweatherFC4d'\n",
    "\n",
    "    try:\n",
    "        suffix = f'{source_file}.csv'\n",
    "        source_df = load_and_merge_files(bucket_name, suffix)\n",
    "        if not source_df.empty:\n",
    "            processed_df = process_dataframe(source_file, source_df)\n",
    "            \n",
    "            # Remove duplicates and empty values\n",
    "            processed_df = processed_df.drop_duplicates().dropna()\n",
    "\n",
    "            # Log the cleaned DataFrame\n",
    "            logger.info(f\"Processed DataFrame for '{dest_df}':\\n{processed_df.head()}\")\n",
    "\n",
    "            # Save DataFrame to S3\n",
    "            save_df_to_s3(bucket_name, processed_df, f'dataframes/{dest_df}.csv')\n",
    "\n",
    "        else:\n",
    "            logger.warning(f\"No data processed for file type '{source_file}'\")\n",
    "\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': 'Data processing completed successfully.'\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data: {str(e)}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': f\"Error processing data: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def load_and_merge_files(bucket_name, suffix):\n",
    "    # Use pagination to list all files with the specified suffix\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name)\n",
    "\n",
    "    files = []\n",
    "    for page in page_iterator:\n",
    "        files.extend([item['Key'] for item in page.get('Contents', []) if item['Key'].endswith(suffix)])\n",
    "    \n",
    "    logger.info(f\"Files found with suffix '{suffix}': {files}\")\n",
    "    \n",
    "    if not files:\n",
    "        logger.warning(f\"No files found with suffix '{suffix}' in bucket '{bucket_name}'.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no files found\n",
    "    \n",
    "    # Load and concatenate all matching files\n",
    "    dataframes = [load_csv_from_s3(bucket_name, file) for file in files]\n",
    "    if not dataframes:\n",
    "        logger.warning(f\"No data loaded for suffix '{suffix}'. Returning an empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "def load_csv_from_s3(bucket_name, file_key):\n",
    "    # Load a CSV file from S3 and return as a DataFrame\n",
    "    logger.info(f\"Loading file '{file_key}' from bucket '{bucket_name}'\")\n",
    "    csv_obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    return pd.read_csv(io.StringIO(csv_string))\n",
    "\n",
    "def process_dataframe(source_file, df):\n",
    "    # Transform the source DataFrame into the required destination DataFrame structure\n",
    "    column_mappings = {\n",
    "        'predictions4days': [\n",
    "            ('city_id', None), \n",
    "            ('city_name', 'venue_city'),\n",
    "            ('coord_lon', 'coord_lon'),\n",
    "            ('coord_lat', 'coord_lat'),\n",
    "            ('country', 'country'),\n",
    "            ('population', 'population'),\n",
    "            ('timezone', 'timezone'),\n",
    "            ('date', 'date'),\n",
    "            ('time', 'time'),\n",
    "            ('temp_day', 'temperature'),\n",
    "            ('pressure', 'pressure'),\n",
    "            ('humidity', 'humidity'),\n",
    "            ('weather_description', 'weather_description'),\n",
    "            ('wind_speed', 'wind_speed'),\n",
    "            ('wind_deg', None),\n",
    "            ('rain', 'rain_volume')\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    if source_file not in column_mappings:\n",
    "        logger.error(f\"No column mappings found for source file '{source_file}'\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    mappings = column_mappings[source_file]\n",
    "    transformed_df = pd.DataFrame()\n",
    "\n",
    "    for mapping in mappings:\n",
    "        if len(mapping) == 2:\n",
    "            source_col, dest_col = mapping\n",
    "        elif len(mapping) == 3:\n",
    "            source_col, dest_col, _ = mapping\n",
    "        else:\n",
    "            logger.error(f\"Invalid column mapping: {mapping}\")\n",
    "            continue\n",
    "\n",
    "        if dest_col:\n",
    "            transformed_df[dest_col] = df[source_col]\n",
    "    \n",
    "    # Filter the DataFrame for dates between 2024-05-01 and 2024-06-07\n",
    "    transformed_df['date'] = pd.to_datetime(transformed_df['date'])  # Ensure 'date' column is in datetime format\n",
    "    filtered_df = transformed_df[(transformed_df['date'] >= '2024-05-01') & (transformed_df['date'] <= '2024-06-07')]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def save_df_to_s3(bucket_name, df, file_key):\n",
    "    # Convert DataFrame to CSV and upload to S3\n",
    "    if df.empty:\n",
    "        logger.warning(f\"DataFrame is empty. Skipping upload for '{file_key}'.\")\n",
    "        return\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=file_key, Body=csv_buffer.getvalue())\n",
    "    logger.info(f\"DataFrame saved to '{file_key}' in bucket '{bucket_name}'\")\n",
    "\n",
    "# This code should be set up in an AWS Lambda function with the necessary IAM permissions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46526ab1",
   "metadata": {},
   "source": [
    "# myDataLoading7: Storing Forecast16days and historic weather into DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce88ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Database connection details from environment variables\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_PORT = int(os.getenv('DB_PORT', 5432))  # Default to 5432 if DB_PORT is not set\n",
    "\n",
    "# Hard-coded database connection details. - TESTDATABASE\n",
    "# DB_HOST = 'database-3.cto9dqorm6ji.us-east-1.rds.amazonaws.com'\n",
    "# DB_NAME = 'databasetest'\n",
    "# DB_USER = 'postgres'\n",
    "# DB_PASSWORD = 'HSLU2024,rai!'\n",
    "# DB_PORT = 5432\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Define S3 bucket and file paths for factweather and factweatherFC16d\n",
    "    bucket_name = 'datalakepartition2'\n",
    "    file_key_factweather = 'dataframes/factweather.csv'\n",
    "    file_key_factweatherFC16d = 'dataframes/factweatherFC16d.csv'\n",
    "\n",
    "    connection = None\n",
    "    try:\n",
    "        # Load the CSV files from S3\n",
    "        factweather_df = load_csv_from_s3(bucket_name, file_key_factweather)\n",
    "        factweatherFC16d_df = load_csv_from_s3(bucket_name, file_key_factweatherFC16d)\n",
    "\n",
    "        # Log DataFrame columns for debugging\n",
    "        logger.info(f\"Columns in factweather_df: {factweather_df.columns.tolist()}\")\n",
    "        logger.info(f\"Columns in factweatherFC16d_df: {factweatherFC16d_df.columns.tolist()}\")\n",
    "\n",
    "        # Convert data types for both DataFrames\n",
    "        factweather_df = convert_factweather_df_types(factweather_df)\n",
    "        factweatherFC16d_df = convert_factweatherFC16d_df_types(factweatherFC16d_df)\n",
    "\n",
    "        # Remove duplicates based on the primary key\n",
    "        factweather_df = remove_duplicates(factweather_df, ['venue_city', 'date', 'time'])\n",
    "        factweatherFC16d_df = remove_duplicates(factweatherFC16d_df, ['venue_city', 'date', 'time'])\n",
    "\n",
    "        # Connect to the PostgreSQL database\n",
    "        connection = connect_to_db()\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Load data into SQL database\n",
    "        load_data_to_sql(factweather_df, cursor, 'factweather')\n",
    "        load_data_to_sql(factweatherFC16d_df, cursor, 'factweatherFC16d')\n",
    "\n",
    "        # Commit the transaction\n",
    "        connection.commit()\n",
    "\n",
    "        logger.info(\"Data loaded into the SQL database successfully.\")\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': 'Data loaded into the SQL database successfully.'\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data into the SQL database: {str(e)}\")\n",
    "        if connection:\n",
    "            connection.rollback()  # Rollback in case of error\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': f\"Error loading data into the SQL database: {str(e)}\"\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "def load_csv_from_s3(bucket_name, file_key):\n",
    "    \"\"\"Load a CSV file from S3 and return as a DataFrame.\"\"\"\n",
    "    logger.info(f\"Loading file '{file_key}' from bucket '{bucket_name}'\")\n",
    "    csv_obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    df = pd.read_csv(io.StringIO(csv_string))  # Load as a DataFrame\n",
    "    return df\n",
    "\n",
    "def connect_to_db():\n",
    "    \"\"\"Connect to the PostgreSQL database using hard-coded details.\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT,\n",
    "            database=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD\n",
    "        )\n",
    "        logger.info(\"Successfully connected to the database\")\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error connecting to the database: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_data_to_sql(df, cursor, table_name):\n",
    "    \"\"\"Load data from DataFrame into SQL table.\"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(f\"The DataFrame for table '{table_name}' is empty. Skipping load.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare SQL query for inserting data\n",
    "    columns = df.columns.tolist()\n",
    "    columns_str = ', '.join(columns)\n",
    "    values_str = ', '.join(['%s'] * len(columns))\n",
    "\n",
    "    insert_query = f\"INSERT INTO {table_name} ({columns_str}) VALUES ({values_str})\"\n",
    "\n",
    "    # Log the SQL query for debugging\n",
    "    logger.info(f\"Insert query for table '{table_name}': {insert_query}\")\n",
    "\n",
    "    # Convert DataFrame rows to list of tuples\n",
    "    data_tuples = [tuple(x) for x in df.to_numpy()]\n",
    "\n",
    "    # Execute the SQL query using batch insert for efficiency\n",
    "    try:\n",
    "        psycopg2.extras.execute_batch(cursor, insert_query, data_tuples)\n",
    "        logger.info(f\"Loaded data into table '{table_name}' successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error executing SQL insert for table '{table_name}': {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def convert_factweather_df_types(df):\n",
    "    \"\"\"Convert data types for the factweather DataFrame.\"\"\"\n",
    "    try:\n",
    "        df['venue_city'] = df['venue_city'].astype(str)\n",
    "        df['temperature'] = df['temperature'].astype(float)\n",
    "        df['pressure'] = df['pressure'].astype(int)\n",
    "        df['humidity'] = df['humidity'].astype(int)\n",
    "        df['wind_speed'] = df['wind_speed'].astype(float)\n",
    "        df['weather_description'] = df['weather_description'].astype(str)\n",
    "        # 'rain_volume' column is not present in factweather, so it's not converted\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['time'] = pd.to_datetime(df['time']).dt.time  # Extract only time part\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting data types for factweather DataFrame: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_factweatherFC16d_df_types(df):\n",
    "    \"\"\"Convert data types for the factweatherFC16d DataFrame.\"\"\"\n",
    "    try:\n",
    "        df['venue_city'] = df['venue_city'].astype(str)\n",
    "        df['coord_lon'] = df['coord_lon'].astype(float)\n",
    "        df['coord_lat'] = df['coord_lat'].astype(float)\n",
    "        df['country'] = df['country'].astype(str)\n",
    "        df['population'] = df['population'].astype(int)\n",
    "        df['timezone'] = df['timezone'].astype(int)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['time'] = pd.to_datetime(df['time']).dt.time  # Extract only time part\n",
    "        df['temperature'] = df['temperature'].astype(float)\n",
    "        df['pressure'] = df['pressure'].astype(int)\n",
    "        df['humidity'] = df['humidity'].astype(int)\n",
    "        df['weather_description'] = df['weather_description'].astype(str)\n",
    "        df['wind_speed'] = df['wind_speed'].astype(float)\n",
    "        df['rain_volume'] = df['rain_volume'].astype(float)  # This column exists in factweatherFC16d\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting data types for factweatherFC16d DataFrame: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return df\n",
    "\n",
    "def remove_duplicates(df, primary_key_columns):\n",
    "    \"\"\"Remove duplicate rows based on primary key columns.\"\"\"\n",
    "    try:\n",
    "        # Remove duplicates based on the primary key columns\n",
    "        df = df.drop_duplicates(subset=primary_key_columns)\n",
    "        logger.info(f\"Removed duplicates based on columns: {primary_key_columns}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error removing duplicates: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7400b",
   "metadata": {},
   "source": [
    "# myDataLoading8: Loading Forecast4days into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1995c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Database connection details from environment variables\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_PORT = int(os.getenv('DB_PORT', 5432))  # Default to 5432 if DB_PORT is not set\n",
    "\n",
    "# Hard-coded database connection details. - TESTDATABASE\n",
    "# DB_HOST = 'database-3.cto9dqorm6ji.us-east-1.rds.amazonaws.com'\n",
    "# DB_NAME = 'databasetest'\n",
    "# DB_USER = 'postgres'\n",
    "# DB_PASSWORD = 'HSLU2024,rai!'\n",
    "# DB_PORT = 5432\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Define S3 bucket and file suffix for factweatherFC4d\n",
    "    bucket_name = 'datalakepartition2'\n",
    "    source_file = 'predictions4days'\n",
    "    dest_df = 'factweatherFC4d'\n",
    "    destination_folder = 'dataframes/'\n",
    "\n",
    "    connection = None\n",
    "    try:\n",
    "        suffix = f'{source_file}.csv'\n",
    "        source_df = load_and_merge_files(bucket_name, suffix)\n",
    "        if not source_df.empty:\n",
    "            processed_df = process_dataframe(source_file, source_df)\n",
    "            \n",
    "            # Remove duplicates based on primary key columns\n",
    "            processed_df = processed_df.drop_duplicates(subset=['venue_city', 'date', 'time']).dropna()\n",
    "\n",
    "            # Log the cleaned DataFrame\n",
    "            logger.info(f\"Processed DataFrame for '{dest_df}':\\n{processed_df.head()}\")\n",
    "\n",
    "            # Save DataFrame to S3\n",
    "            save_df_to_s3(bucket_name, processed_df, f'{destination_folder}{dest_df}.csv')\n",
    "\n",
    "            # Connect to the PostgreSQL database\n",
    "            connection = connect_to_db()\n",
    "            cursor = connection.cursor()\n",
    "\n",
    "            # Load data into the factweatherFC4d table in SQL database\n",
    "            load_data_to_sql(processed_df, cursor, 'factweatherFC4d')\n",
    "\n",
    "            # Commit the transaction\n",
    "            connection.commit()\n",
    "\n",
    "            logger.info(\"Data loaded into the SQL database successfully.\")\n",
    "            return {\n",
    "                'statusCode': 200,\n",
    "                'body': 'Data processing and loading to SQL completed successfully.'\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            logger.warning(f\"No data processed for file type '{source_file}'\")\n",
    "\n",
    "            return {\n",
    "                'statusCode': 204,\n",
    "                'body': 'No data processed because no files were found.'\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing data: {str(e)}\")\n",
    "        if connection:\n",
    "            connection.rollback()  # Rollback in case of error\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': f\"Error processing data: {str(e)}\"\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "def load_and_merge_files(bucket_name, suffix):\n",
    "    # Use pagination to list all files with the specified suffix\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name)\n",
    "\n",
    "    files = []\n",
    "    for page in page_iterator:\n",
    "        files.extend([item['Key'] for item in page.get('Contents', []) if item['Key'].endswith(suffix)])\n",
    "    \n",
    "    logger.info(f\"Files found with suffix '{suffix}': {files}\")\n",
    "    \n",
    "    if not files:\n",
    "        logger.warning(f\"No files found with suffix '{suffix}' in bucket '{bucket_name}'.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no files found\n",
    "    \n",
    "    # Load and concatenate all matching files\n",
    "    dataframes = [load_csv_from_s3(bucket_name, file) for file in files]\n",
    "    if not dataframes:\n",
    "        logger.warning(f\"No data loaded for suffix '{suffix}'. Returning an empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "def load_csv_from_s3(bucket_name, file_key):\n",
    "    # Load a CSV file from S3 and return as a DataFrame\n",
    "    logger.info(f\"Loading file '{file_key}' from bucket '{bucket_name}'\")\n",
    "    csv_obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    return pd.read_csv(io.StringIO(csv_string))\n",
    "\n",
    "def process_dataframe(source_file, df):\n",
    "    # Transform the source DataFrame into the required destination DataFrame structure\n",
    "    column_mappings = {\n",
    "        'predictions4days': [\n",
    "            ('city_id', None), \n",
    "            ('city_name', 'venue_city'),\n",
    "            ('coord_lon', 'coord_lon'),\n",
    "            ('coord_lat', 'coord_lat'),\n",
    "            ('country', 'country'),\n",
    "            ('population', 'population'),\n",
    "            ('timezone', 'timezone'),\n",
    "            ('date', 'date'),\n",
    "            ('time', 'time'),\n",
    "            ('temp_day', 'temperature'),\n",
    "            ('pressure', 'pressure'),\n",
    "            ('humidity', 'humidity'),\n",
    "            ('weather_description', 'weather_description'),\n",
    "            ('wind_speed', 'wind_speed'),\n",
    "            ('wind_deg', None),\n",
    "            ('rain', 'rain_volume')\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    if source_file not in column_mappings:\n",
    "        logger.error(f\"No column mappings found for source file '{source_file}'\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    mappings = column_mappings[source_file]\n",
    "    transformed_df = pd.DataFrame()\n",
    "\n",
    "    for mapping in mappings:\n",
    "        if len(mapping) == 2:\n",
    "            source_col, dest_col = mapping\n",
    "        elif len(mapping) == 3:\n",
    "            source_col, dest_col, _ = mapping\n",
    "        else:\n",
    "            logger.error(f\"Invalid column mapping: {mapping}\")\n",
    "            continue\n",
    "\n",
    "        if dest_col:\n",
    "            transformed_df[dest_col] = df[source_col]\n",
    "    \n",
    "    # Filter the DataFrame for dates between 2024-05-01 and 2024-06-07\n",
    "    transformed_df['date'] = pd.to_datetime(transformed_df['date'])  # Ensure 'date' column is in datetime format\n",
    "    filtered_df = transformed_df[(transformed_df['date'] >= '2024-05-01') & (transformed_df['date'] <= '2024-06-07')]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def save_df_to_s3(bucket_name, df, file_key):\n",
    "    # Convert DataFrame to CSV and upload to S3\n",
    "    if df.empty:\n",
    "        logger.warning(f\"DataFrame is empty. Skipping upload for '{file_key}'.\")\n",
    "        return\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=file_key, Body=csv_buffer.getvalue())\n",
    "    logger.info(f\"DataFrame saved to '{file_key}' in bucket '{bucket_name}'\")\n",
    "\n",
    "def connect_to_db():\n",
    "    \"\"\"Connect to the PostgreSQL database using hard-coded details.\"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT,\n",
    "            database=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD\n",
    "        )\n",
    "        logger.info(\"Successfully connected to the database\")\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error connecting to the database: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_data_to_sql(df, cursor, table_name):\n",
    "    \"\"\"Load data from DataFrame into SQL table.\"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(f\"The DataFrame for table '{table_name}' is empty. Skipping load.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare SQL query for inserting data\n",
    "    columns = df.columns.tolist()\n",
    "    columns_str = ', '.join(columns)\n",
    "    values_str = ', '.join(['%s'] * len(columns))\n",
    "\n",
    "    # Extract unique primary keys from the DataFrame\n",
    "    primary_key_columns = ['venue_city', 'date', 'time']\n",
    "    unique_keys = df[primary_key_columns].drop_duplicates()\n",
    "\n",
    "    # Delete existing rows that match the primary keys\n",
    "    for index, row in unique_keys.iterrows():\n",
    "        delete_query = f\"DELETE FROM {table_name} WHERE venue_city = %s AND date = %s AND time = %s\"\n",
    "        try:\n",
    "            cursor.execute(delete_query, (row['venue_city'], row['date'], row['time']))\n",
    "            logger.info(f\"Deleted existing rows for key: {row['venue_city']}, {row['date']}, {row['time']}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting existing row: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    insert_query = f\"INSERT INTO {table_name} ({columns_str}) VALUES ({values_str})\"\n",
    "\n",
    "    # Log the SQL query for debugging\n",
    "    logger.info(f\"Insert query for table '{table_name}': {insert_query}\")\n",
    "\n",
    "    # Convert DataFrame rows to list of tuples\n",
    "    data_tuples = [tuple(x) for x in df.to_numpy()]\n",
    "\n",
    "    # Execute the SQL query using batch insert for efficiency\n",
    "    try:\n",
    "        psycopg2.extras.execute_batch(cursor, insert_query, data_tuples)\n",
    "        logger.info(f\"Loaded data into table '{table_name}' successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error executing SQL insert for table '{table_name}': {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# This code should be set up in an AWS Lambda function with the necessary IAM permissions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
