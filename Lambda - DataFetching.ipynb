{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d0fa73",
   "metadata": {},
   "source": [
    "# myDataFetching1: STATISTICS_PLAYERS_ALL.JSON into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    start_time = time.time()\n",
    "    source_bucket_name = 'dwlprojectbucket'\n",
    "    target_bucket_name = 'datalakepartition2'\n",
    "    \n",
    "    # Only process statistics_players_all.json files\n",
    "    pattern = 'statistics_players_all.json'\n",
    "    \n",
    "    # List all objects in the source bucket matching the pattern\n",
    "    files_to_process = list_objects_with_pagination(source_bucket_name, pattern)\n",
    "    print(f\"Time to list objects in source bucket: {time.time() - start_time} seconds\")\n",
    "    \n",
    "    if not files_to_process:\n",
    "        print(\"No files to process that match the pattern.\")\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps('No files to process that match the pattern.')\n",
    "        }\n",
    "    \n",
    "    # Use a ThreadPoolExecutor for concurrent processing\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for key in files_to_process:\n",
    "            # Extract the date and time from the filename and ensure it is formatted correctly\n",
    "            date_time = key.split(' - ')[0].strip()  # Adjust this split based on your actual filename pattern\n",
    "            output_file_key = f'{date_time} - {pattern.split(\".\")[0]}.csv'\n",
    "            \n",
    "            # Schedule the file processing\n",
    "            futures.append(executor.submit(process_file_if_not_exists, source_bucket_name, target_bucket_name, key, transform_statistics_players_all, output_file_key))\n",
    "        \n",
    "        # Wait for all futures to complete\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "    print(f\"Total execution time: {time.time() - start_time} seconds\")\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('CSV files created successfully')\n",
    "    }\n",
    "\n",
    "def process_file_if_not_exists(source_bucket_name, target_bucket_name, key, transform_func, output_file_key):\n",
    "    try:\n",
    "        # Process the file regardless of its existence in the target bucket\n",
    "        print(f\"Processing file: {key}\")\n",
    "        process_file(source_bucket_name, target_bucket_name, key, transform_func, output_file_key)\n",
    "        \n",
    "        # Load the data into a DataFrame and inspect it\n",
    "        obj = s3.get_object(Bucket=source_bucket_name, Key=key)\n",
    "        raw_data = obj['Body'].read().decode('utf-8').splitlines()\n",
    "        \n",
    "        # Apply the transformation function to get the DataFrame\n",
    "        extracted_df = transform_func(raw_data)\n",
    "        \n",
    "        # Display the DataFrame\n",
    "        print(f\"DataFrame loaded for file {key}:\\n\", extracted_df.head())  # Show the first few rows for inspection\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {key}: {e}\")\n",
    "\n",
    "def check_file_exists(bucket_name, key):\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket_name, Key=key)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def list_objects_with_pagination(bucket_name, pattern):\n",
    "    files_to_process = []\n",
    "    continuation_token = None\n",
    "    \n",
    "    while True:\n",
    "        if continuation_token:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name, ContinuationToken=continuation_token)\n",
    "        else:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "        \n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                if obj['Key'].endswith(pattern):\n",
    "                    files_to_process.append(obj['Key'])\n",
    "        \n",
    "        if response.get('IsTruncated'):\n",
    "            continuation_token = response.get('NextContinuationToken')\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return files_to_process\n",
    "\n",
    "def process_file(source_bucket_name, target_bucket_name, key, transform_func, output_file_key):\n",
    "    try:\n",
    "        # Get the file from the source S3 bucket\n",
    "        fetch_start_time = time.time()\n",
    "        obj = s3.get_object(Bucket=source_bucket_name, Key=key)\n",
    "        print(f\"Time to fetch file {key}: {time.time() - fetch_start_time} seconds\")\n",
    "\n",
    "        read_start_time = time.time()\n",
    "        raw_data = obj['Body'].read().decode('utf-8').splitlines()\n",
    "        print(f\"Time to read and decode file {key}: {time.time() - read_start_time} seconds\")\n",
    "        \n",
    "        # Apply the transformation function\n",
    "        transform_start_time = time.time()\n",
    "        extracted_df = transform_func(raw_data)\n",
    "        print(f\"Time to transform file {key}: {time.time() - transform_start_time} seconds\")\n",
    "        \n",
    "        if extracted_df.empty:\n",
    "            print(f\"No data extracted from file {key}\")\n",
    "            return\n",
    "\n",
    "        # Convert DataFrame to CSV\n",
    "        csv_buffer = StringIO()\n",
    "        csv_start_time = time.time()\n",
    "        extracted_df.to_csv(csv_buffer, index=False)\n",
    "        print(f\"Time to convert DataFrame to CSV for file {key}: {time.time() - csv_start_time} seconds\")\n",
    "        \n",
    "        # Upload the CSV to the target S3 bucket\n",
    "        upload_start_time = time.time()\n",
    "        s3.put_object(Bucket=target_bucket_name, Key=output_file_key, Body=csv_buffer.getvalue())\n",
    "        print(f\"Time to upload CSV for file {key}: {time.time() - upload_start_time} seconds\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {key}: {e}\")\n",
    "\n",
    "# Transformation function for statistics_players_all.json\n",
    "\n",
    "def transform_statistics_players_all(raw_data):\n",
    "    extracted_data = []\n",
    "    \n",
    "    for line in raw_data:\n",
    "        try:\n",
    "            # First parse the line to get the list of JSON strings\n",
    "            outer_list = json.loads(line)\n",
    "            for json_str in outer_list:\n",
    "                # Then parse each JSON string in the list\n",
    "                fixture = json.loads(json_str)\n",
    "                fixture_id = fixture['parameters']['fixture']\n",
    "                for team_data in fixture['response']:\n",
    "                    team = team_data['team']\n",
    "                    team_id = team['id']\n",
    "                    team_name = team['name']\n",
    "                    \n",
    "                    for player_data in team_data['players']:\n",
    "                        player = player_data['player']\n",
    "                        statistics = player_data['statistics'][0]  # assuming there's always one statistics entry per player\n",
    "\n",
    "                        player_id = player['id']\n",
    "                        player_name = player['name']\n",
    "                        \n",
    "                        extracted_data.append({\n",
    "                            'fixtureID': fixture_id,\n",
    "                            'team_Id': team_id,\n",
    "                            'team_name': team_name,\n",
    "                            'player_id': player_id,\n",
    "                            'player_name': player_name,\n",
    "                            'minutes': statistics['games'].get('minutes'),\n",
    "                            'number': statistics['games'].get('number'),\n",
    "                            'position': statistics['games'].get('position'),\n",
    "                            'rating': statistics['games'].get('rating'),\n",
    "                            'captain': statistics['games'].get('captain'),\n",
    "                            'substitute': statistics['games'].get('substitute'),\n",
    "                            'offsides': statistics.get('offsides'),\n",
    "                            'shots_total': statistics['shots'].get('total'),\n",
    "                            'shots_on': statistics['shots'].get('on'),\n",
    "                            'goals_total': statistics['goals'].get('total'),\n",
    "                            'goals_conceded': statistics['goals'].get('conceded'),\n",
    "                            'goals_assists': statistics['goals'].get('assists'),\n",
    "                            'saves': statistics['goals'].get('saves'),\n",
    "                            'passes_total': statistics['passes'].get('total'),\n",
    "                            'passes_key': statistics['passes'].get('key'),\n",
    "                            'passes_accuracy': statistics['passes'].get('accuracy'),\n",
    "                            'tackles_total': statistics['tackles'].get('total'),\n",
    "                            'tackles_blocks': statistics['tackles'].get('blocks'),\n",
    "                            'tackles_interceptions': statistics['tackles'].get('interceptions'),\n",
    "                            'duels_total': statistics['duels'].get('total'),\n",
    "                            'duels_won': statistics['duels'].get('won'),\n",
    "                            'dribbles_attempts': statistics['dribbles'].get('attempts'),\n",
    "                            'dribbles_success': statistics['dribbles'].get('success'),\n",
    "                            'dribbles_past': statistics['dribbles'].get('past'),\n",
    "                            'fouls_drawn': statistics['fouls'].get('drawn'),\n",
    "                            'fouls_committed': statistics['fouls'].get('committed'),\n",
    "                            'yellow_cards': statistics['cards'].get('yellow'),\n",
    "                            'red_cards': statistics['cards'].get('red'),\n",
    "                            'penalty_won': statistics['penalty'].get('won'),\n",
    "                            'penalty_committed': statistics['penalty'].get('commited'),\n",
    "                            'penalty_scored': statistics['penalty'].get('scored'),\n",
    "                            'penalty_missed': statistics['penalty'].get('missed'),\n",
    "                            'penalty_saved': statistics['penalty'].get('saved')\n",
    "                        })\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame(extracted_data)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4670d6",
   "metadata": {},
   "source": [
    "# myDataFetching2: PREDICTIONS16DAYS.JSON into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    source_bucket_name = 'dwlprojectbucket'\n",
    "    target_bucket_name = 'datalakepartition2'\n",
    "    \n",
    "    # Get the list of existing keys in the target bucket\n",
    "    existing_keys = get_existing_keys(target_bucket_name)\n",
    "    \n",
    "    # List all objects in the source bucket\n",
    "    response = s3.list_objects_v2(Bucket=source_bucket_name)\n",
    "    for obj in response.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        # Process only files that match the specified pattern and do not exist in the target bucket\n",
    "        if key.endswith('predictions16days.json'):\n",
    "            # Extract date and time from the file name\n",
    "            date_time = key.split(' - ')[0]\n",
    "            output_file_key = f'{date_time} - predictions16days.csv'\n",
    "            if output_file_key not in existing_keys:\n",
    "                process_file(source_bucket_name, target_bucket_name, key, transform_predictions_16days, output_file_key)\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('CSV file created successfully for predictions16days.json')\n",
    "    }\n",
    "\n",
    "def get_existing_keys(bucket_name):\n",
    "    existing_keys = set()\n",
    "    continuation_token = None\n",
    "    while True:\n",
    "        if continuation_token:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name, ContinuationToken=continuation_token)\n",
    "        else:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                existing_keys.add(obj['Key'])\n",
    "        if response.get('IsTruncated'):\n",
    "            continuation_token = response.get('NextContinuationToken')\n",
    "        else:\n",
    "            break\n",
    "    return existing_keys\n",
    "\n",
    "def process_file(source_bucket_name, target_bucket_name, key, transform_func, output_file_key):\n",
    "    try:\n",
    "        # Get the file from the source S3 bucket\n",
    "        obj = s3.get_object(Bucket=source_bucket_name, Key=key)\n",
    "        raw_data = obj['Body'].read().decode('utf-8').splitlines()\n",
    "        \n",
    "        # Apply the transformation function\n",
    "        extracted_df = transform_func(raw_data)\n",
    "        \n",
    "        # Convert DataFrame to CSV\n",
    "        csv_buffer = StringIO()\n",
    "        extracted_df.to_csv(csv_buffer, index=False)\n",
    "        \n",
    "        # Upload the CSV to the target S3 bucket\n",
    "        s3.put_object(Bucket=target_bucket_name, Key=output_file_key, Body=csv_buffer.getvalue())\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {key}: {e}\")\n",
    "\n",
    "def transform_predictions_16days(raw_data):\n",
    "    data = json.loads('\\n'.join(raw_data))  # Combine lines and load JSON\n",
    "    extracted_data = []\n",
    "\n",
    "    def safe_get(d, keys, default=None):\n",
    "        for key in keys:\n",
    "            try:\n",
    "                d = d[key]\n",
    "            except (KeyError, IndexError, TypeError):\n",
    "                return default\n",
    "        return d\n",
    "\n",
    "    def kelvin_to_celsius(kelvin):\n",
    "        return kelvin - 273.15\n",
    "\n",
    "    for city_data in data:\n",
    "        city_info = city_data['city']\n",
    "        city_id = city_info['id']\n",
    "        city_name = city_info['name']\n",
    "        coord_lon = city_info['coord']['lon']\n",
    "        coord_lat = city_info['coord']['lat']\n",
    "        country = city_info['country']\n",
    "        population = city_info['population']\n",
    "        timezone = city_info['timezone']\n",
    "        \n",
    "        weather_list = city_data['list']\n",
    "        \n",
    "        for weather in weather_list:\n",
    "            dt = safe_get(weather, ['dt'])\n",
    "            temp_day_kelvin = safe_get(weather, ['temp', 'day'])\n",
    "            pressure = safe_get(weather, ['pressure'])\n",
    "            humidity = safe_get(weather, ['humidity'])\n",
    "            weather_description = safe_get(weather, ['weather', 0, 'description'])\n",
    "            wind_speed = safe_get(weather, ['speed'])\n",
    "            wind_deg = safe_get(weather, ['deg'])\n",
    "            rain = safe_get(weather, ['rain'], 0)\n",
    "\n",
    "            # Convert temperatures from Kelvin to Celsius\n",
    "            temp_day_celsius = kelvin_to_celsius(temp_day_kelvin) if temp_day_kelvin is not None else None\n",
    "            \n",
    "            # Convert Unix timestamp to human-readable format\n",
    "            if dt is not None:\n",
    "                dt_obj = datetime.utcfromtimestamp(dt)\n",
    "                date_str = dt_obj.date().isoformat()\n",
    "                time_str = dt_obj.time().isoformat()\n",
    "            else:\n",
    "                date_str = time_str = None\n",
    "            \n",
    "            extracted_data.append({\n",
    "                'city_id': city_id,\n",
    "                'city_name': city_name,\n",
    "                'coord_lon': coord_lon,\n",
    "                'coord_lat': coord_lat,\n",
    "                'country': country,\n",
    "                'population': population,\n",
    "                'timezone': timezone,\n",
    "                'date': date_str,\n",
    "                'time': time_str,\n",
    "                'temp_day': temp_day_celsius,\n",
    "                'pressure': pressure,\n",
    "                'humidity': humidity,\n",
    "                'weather_description': weather_description,\n",
    "                'wind_speed': wind_speed,\n",
    "                'wind_deg': wind_deg,\n",
    "                'rain': rain\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(extracted_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a4232",
   "metadata": {},
   "source": [
    "#  myDataFetching3: PREDICTIONS4DAYS.JSON into CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0cb303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    source_bucket_name = 'dwlprojectbucket'\n",
    "    target_bucket_name = 'datalakepartition2'\n",
    "    \n",
    "    # Get the list of existing keys in the target bucket\n",
    "    existing_keys = get_existing_keys(target_bucket_name)\n",
    "    print(f\"Existing keys in target bucket: {existing_keys}\")\n",
    "    \n",
    "    # List all objects in the source bucket\n",
    "    response = s3.list_objects_v2(Bucket=source_bucket_name)\n",
    "    \n",
    "    if 'Contents' not in response:\n",
    "        print(\"No files found in the source bucket.\")\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps('No files found in the source bucket.')\n",
    "        }\n",
    "    \n",
    "    for obj in response.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        print(f\"Found file in source bucket: {key}\")\n",
    "        # Process only files that match the specified pattern and do not exist in the target bucket\n",
    "        if key.endswith('predictions4days.json'):\n",
    "            # Extract date and time from the file name\n",
    "            date_time = key.split(' - ')[0]\n",
    "            output_file_key = f'{date_time} - predictions4days.csv'\n",
    "            if output_file_key not in existing_keys:\n",
    "                print(f\"Processing file: {key}\")\n",
    "                process_file(source_bucket_name, target_bucket_name, key, transform_predictions_4d, output_file_key)\n",
    "            else:\n",
    "                print(f\"Skipping already existing file: {output_file_key}\")\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('CSV file created successfully for predictions4days.json')\n",
    "    }\n",
    "\n",
    "def get_existing_keys(bucket_name):\n",
    "    existing_keys = set()\n",
    "    continuation_token = None\n",
    "    while True:\n",
    "        if continuation_token:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name, ContinuationToken=continuation_token)\n",
    "        else:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                existing_keys.add(obj['Key'])\n",
    "        if response.get('IsTruncated'):\n",
    "            continuation_token = response.get('NextContinuationToken')\n",
    "        else:\n",
    "            break\n",
    "    return existing_keys\n",
    "\n",
    "def process_file(source_bucket_name, target_bucket_name, key, transform_func, output_file_key):\n",
    "    try:\n",
    "        # Get the file from the source S3 bucket\n",
    "        print(f\"Downloading file: {key} from bucket: {source_bucket_name}\")\n",
    "        obj = s3.get_object(Bucket=source_bucket_name, Key=key)\n",
    "        raw_data = obj['Body'].read().decode('utf-8').splitlines()\n",
    "        \n",
    "        # Apply the transformation function\n",
    "        extracted_df = transform_func(raw_data)\n",
    "        print(f\"DataFrame created with {len(extracted_df)} records\")\n",
    "        \n",
    "        # Convert DataFrame to CSV\n",
    "        csv_buffer = StringIO()\n",
    "        extracted_df.to_csv(csv_buffer, index=False)\n",
    "        \n",
    "        # Upload the CSV to the target S3 bucket\n",
    "        print(f\"Uploading transformed CSV to bucket: {target_bucket_name} with key: {output_file_key}\")\n",
    "        s3.put_object(Bucket=target_bucket_name, Key=output_file_key, Body=csv_buffer.getvalue())\n",
    "        print(f\"File {output_file_key} uploaded to {target_bucket_name}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {key}: {e}\")\n",
    "\n",
    "def transform_predictions_4d(raw_data):\n",
    "    data = json.loads('\\n'.join(raw_data))  # Combine lines and load JSON\n",
    "    extracted_data = []\n",
    "\n",
    "    def safe_get(d, keys, default=None):\n",
    "        for key in keys:\n",
    "            try:\n",
    "                d = d[key]\n",
    "            except (KeyError, IndexError, TypeError):\n",
    "                return default\n",
    "        return d\n",
    "\n",
    "    def kelvin_to_celsius(kelvin):\n",
    "        return kelvin - 273.15\n",
    "\n",
    "    for city_data in data:\n",
    "        city_info = city_data['city']\n",
    "        city_id = city_info['id']\n",
    "        city_name = city_info['name']\n",
    "        coord_lon = city_info['coord']['lon']\n",
    "        coord_lat = city_info['coord']['lat']\n",
    "        country = city_info['country']\n",
    "        population = city_info['population']\n",
    "        timezone = city_info['timezone']\n",
    "        \n",
    "        weather_list = city_data['list']\n",
    "        \n",
    "        for weather in weather_list:\n",
    "            dt = safe_get(weather, ['dt'])\n",
    "            temp_day_kelvin = safe_get(weather, ['main', 'temp'])\n",
    "            pressure = safe_get(weather, ['main', 'pressure'])\n",
    "            humidity = safe_get(weather, ['main', 'humidity'])\n",
    "            weather_description = safe_get(weather, ['weather', 0, 'description'])\n",
    "            wind_speed = safe_get(weather, ['wind', 'speed'])\n",
    "            wind_deg = safe_get(weather, ['wind', 'deg'])\n",
    "            rain = safe_get(weather, ['rain', '1h'], 0)\n",
    "\n",
    "            # Convert temperatures from Kelvin to Celsius\n",
    "            temp_day_celsius = kelvin_to_celsius(temp_day_kelvin) if temp_day_kelvin is not None else None\n",
    "            \n",
    "            # Convert Unix timestamp to human-readable format\n",
    "            if dt is not None:\n",
    "                dt_obj = datetime.utcfromtimestamp(dt)\n",
    "                date_str = dt_obj.date().isoformat()\n",
    "                time_str = dt_obj.time().isoformat()\n",
    "            else:\n",
    "                date_str = time_str = None\n",
    "            \n",
    "            extracted_data.append({\n",
    "                'city_id': city_id,\n",
    "                'city_name': city_name,\n",
    "                'coord_lon': coord_lon,\n",
    "                'coord_lat': coord_lat,\n",
    "                'country': country,\n",
    "                'population': population,\n",
    "                'timezone': timezone,\n",
    "                'date': date_str,\n",
    "                'time': time_str,\n",
    "                'temp_day': temp_day_celsius,\n",
    "                'pressure': pressure,\n",
    "                'humidity': humidity,\n",
    "                'weather_description': weather_description,\n",
    "                'wind_speed': wind_speed,\n",
    "                'wind_deg': wind_deg,\n",
    "                'rain': rain\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(extracted_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b837a4",
   "metadata": {},
   "source": [
    "# myDataFetching4: HISTORIC_WEATHER.JSON into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac31f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import time\n",
    "from datetime import datetime  # Importing datetime module\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    start_time = time.time()\n",
    "    source_bucket_name = 'dwlprojectbucket'\n",
    "    target_bucket_name = 'datalakepartition2'\n",
    "    target_file_key = 'historic_weather.csv'  # Output file name in the target bucket\n",
    "    \n",
    "    file_to_process = 'historic_weather.json'\n",
    "    \n",
    "    try:\n",
    "        # Pagination setup\n",
    "        continuation_token = None\n",
    "        file_found = False\n",
    "        \n",
    "        while True:\n",
    "            # List objects in the source bucket with pagination handling\n",
    "            if continuation_token:\n",
    "                response = s3.list_objects_v2(Bucket=source_bucket_name, ContinuationToken=continuation_token)\n",
    "            else:\n",
    "                response = s3.list_objects_v2(Bucket=source_bucket_name)\n",
    "\n",
    "            print(f\"Time to list objects in source bucket: {time.time() - start_time} seconds\")\n",
    "\n",
    "            if 'Contents' not in response:\n",
    "                print(\"No files found in source bucket.\")\n",
    "                break\n",
    "\n",
    "            # Print all the keys found in the bucket\n",
    "            print(\"Listing all files in the source bucket:\")\n",
    "            for obj in response.get('Contents', []):\n",
    "                print(f\"Found file: {obj['Key']}\")\n",
    "                \n",
    "                # Check if the specific file exists\n",
    "                if obj['Key'] == file_to_process:\n",
    "                    file_found = True\n",
    "                    print(f\"Found file to process: {obj['Key']}\")\n",
    "                    \n",
    "                    # Process the file\n",
    "                    process_file(source_bucket_name, target_bucket_name, obj['Key'], target_file_key)\n",
    "                    break\n",
    "\n",
    "            if file_found:\n",
    "                break  # Exit the loop if file is found\n",
    "\n",
    "            # Check if there are more pages of results\n",
    "            if response.get('IsTruncated'):\n",
    "                continuation_token = response.get('NextContinuationToken')\n",
    "            else:\n",
    "                break  # No more pages\n",
    "\n",
    "        if not file_found:\n",
    "            print(f\"File {file_to_process} not found in source bucket.\")\n",
    "            return {\n",
    "                'statusCode': 200,\n",
    "                'body': json.dumps(f'File {file_to_process} not found in source bucket.')\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps(f\"Error during processing: {e}\")\n",
    "        }\n",
    "    \n",
    "    print(f\"Total execution time: {time.time() - start_time} seconds\")\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(f'CSV file created successfully for {file_to_process}')\n",
    "    }\n",
    "\n",
    "def process_file(source_bucket_name, target_bucket_name, key, output_file_key):\n",
    "    try:\n",
    "        # Get the file from the source S3 bucket\n",
    "        print(f\"Fetching file {key} from bucket {source_bucket_name}...\")\n",
    "        obj = s3.get_object(Bucket=source_bucket_name, Key=key)\n",
    "\n",
    "        raw_data = obj['Body'].read().decode('utf-8')\n",
    "        print(f\"Fetched and read file {key}\")\n",
    "\n",
    "        # Transform the JSON data to a DataFrame\n",
    "        data = json.loads(raw_data)\n",
    "        extracted_df = transform_historic_weather(data)\n",
    "        print(f\"Transformed data to DataFrame with shape {extracted_df.shape}\")\n",
    "\n",
    "        # Convert DataFrame to CSV\n",
    "        csv_buffer = StringIO()\n",
    "        extracted_df.to_csv(csv_buffer, index=False)\n",
    "        print(f\"Converted DataFrame to CSV\")\n",
    "\n",
    "        # Upload the CSV to the target S3 bucket\n",
    "        s3.put_object(Bucket=target_bucket_name, Key=output_file_key, Body=csv_buffer.getvalue())\n",
    "        print(f\"Uploaded CSV to bucket {target_bucket_name} with key {output_file_key}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {key}: {e}\")\n",
    "        raise e\n",
    "\n",
    "def transform_historic_weather(data):\n",
    "    extracted_data = []\n",
    "\n",
    "    def safe_get(d, keys, default=None):\n",
    "        for key in keys:\n",
    "            try:\n",
    "                d = d[key]\n",
    "            except (KeyError, IndexError, TypeError):\n",
    "                return default\n",
    "        return d\n",
    "\n",
    "    def kelvin_to_celsius(kelvin):\n",
    "        return kelvin - 273.15\n",
    "\n",
    "    for city_data in data:\n",
    "        city_id = city_data.get('city_id')\n",
    "        weather_list = city_data.get('list', [])\n",
    "        \n",
    "        for weather in weather_list:\n",
    "            dt = safe_get(weather, ['dt'])\n",
    "            main = safe_get(weather, ['main'], {})\n",
    "            wind = safe_get(weather, ['wind'], {})\n",
    "            clouds = safe_get(weather, ['clouds', 'all'], None)\n",
    "            weather_description = safe_get(weather, ['weather', 0, 'description'], None)\n",
    "            \n",
    "            temp_celsius = kelvin_to_celsius(main.get('temp')) if main.get('temp') is not None else None\n",
    "            \n",
    "            if dt is not None:\n",
    "                dt_obj = datetime.utcfromtimestamp(dt)  # Use the imported datetime module\n",
    "                date_str = dt_obj.date().isoformat()\n",
    "                time_str = dt_obj.time().isoformat()\n",
    "                datetime_str = dt_obj.isoformat()\n",
    "            else:\n",
    "                date_str = time_str = datetime_str = None\n",
    "            \n",
    "            extracted_data.append({\n",
    "                'city_id': city_id,\n",
    "                'datetime': datetime_str,\n",
    "                'date': date_str,\n",
    "                'time': time_str,\n",
    "                'temp': temp_celsius,\n",
    "                'pressure': main.get('pressure'),\n",
    "                'humidity': main.get('humidity'),\n",
    "                'wind_speed': wind.get('speed'),\n",
    "                'clouds': clouds,\n",
    "                'weather_description': weather_description\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(extracted_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f3c331",
   "metadata": {},
   "source": [
    "# myDataFetching5: FIXTURES.JSON into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f24710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    start_time = time.time()\n",
    "    source_bucket_name = 'dwlprojectbucket'\n",
    "    target_bucket_name = 'datalakepartition2'\n",
    "    \n",
    "    # Only process fixtures.json files\n",
    "    file_transformations = {\n",
    "        'fixtures.json': transform_fixtures,\n",
    "    }\n",
    "    \n",
    "    # Get the list of existing keys in the target bucket\n",
    "    existing_keys = get_existing_keys(target_bucket_name)\n",
    "    print(f\"Time to fetch existing keys: {time.time() - start_time} seconds\")\n",
    "    print(f\"Existing keys in target bucket: {existing_keys}\")\n",
    "    \n",
    "    # List all objects in the source bucket\n",
    "    response = s3.list_objects_v2(Bucket=source_bucket_name)\n",
    "    print(f\"Time to list objects in source bucket: {time.time() - start_time} seconds\")\n",
    "    if 'Contents' not in response:\n",
    "        print(\"No files found in source bucket.\")\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps('No files found in source bucket.')\n",
    "        }\n",
    "    \n",
    "    for obj in response.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        # Process only files that match the specified pattern and do not exist in the target bucket\n",
    "        for pattern, transform_func in file_transformations.items():\n",
    "            if key.endswith(pattern):\n",
    "                # Extract the date and time from the filename and ensure it is formatted correctly\n",
    "                date_time = key.split(' - ')[0].strip()  # Adjust this split based on your actual filename pattern\n",
    "                output_file_key = f'{date_time} - {pattern.split(\".\")[0]}.csv'\n",
    "                if output_file_key not in existing_keys:\n",
    "                    print(f\"Processing file: {key}\")\n",
    "                    process_file(source_bucket_name, target_bucket_name, key, transform_func, output_file_key)\n",
    "\n",
    "    print(f\"Total execution time: {time.time() - start_time} seconds\")\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('CSV files created successfully')\n",
    "    }\n",
    "\n",
    "def get_existing_keys(bucket_name):\n",
    "    existing_keys = set()\n",
    "    continuation_token = None\n",
    "    while True:\n",
    "        if continuation_token:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name, ContinuationToken=continuation_token)\n",
    "        else:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                existing_keys.add(obj['Key'])\n",
    "        if response.get('IsTruncated'):\n",
    "            continuation_token = response.get('NextContinuationToken')\n",
    "        else:\n",
    "            break\n",
    "    return existing_keys\n",
    "\n",
    "def process_file(source_bucket_name, target_bucket_name, key, transform_func, output_file_key):\n",
    "    try:\n",
    "        # Get the file from the source S3 bucket\n",
    "        fetch_start_time = time.time()\n",
    "        obj = s3.get_object(Bucket=source_bucket_name, Key=key)\n",
    "        print(f\"Time to fetch file {key}: {time.time() - fetch_start_time} seconds\")\n",
    "\n",
    "        read_start_time = time.time()\n",
    "        raw_data = obj['Body'].read().decode('utf-8').splitlines()\n",
    "        print(f\"Time to read and decode file {key}: {time.time() - read_start_time} seconds\")\n",
    "        \n",
    "        # Apply the transformation function\n",
    "        transform_start_time = time.time()\n",
    "        extracted_df = transform_func(raw_data)\n",
    "        print(f\"Time to transform file {key}: {time.time() - transform_start_time} seconds\")\n",
    "        \n",
    "        # Convert DataFrame to CSV\n",
    "        csv_buffer = StringIO()\n",
    "        csv_start_time = time.time()\n",
    "        extracted_df.to_csv(csv_buffer, index=False)\n",
    "        print(f\"Time to convert DataFrame to CSV for file {key}: {time.time() - csv_start_time} seconds\")\n",
    "        \n",
    "        # Upload the CSV to the target S3 bucket\n",
    "        upload_start_time = time.time()\n",
    "        s3.put_object(Bucket=target_bucket_name, Key=output_file_key, Body=csv_buffer.getvalue())\n",
    "        print(f\"Time to upload CSV for file {key}: {time.time() - upload_start_time} seconds\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {key}: {e}\")\n",
    "\n",
    "# Transformation function for fixtures.json\n",
    "\n",
    "def transform_fixtures(raw_data):\n",
    "    data = json.loads('\\n'.join(raw_data))  # Combine lines and load JSON\n",
    "    extracted_data = []\n",
    "\n",
    "    def safe_get(d, keys, default=None):\n",
    "        for key in keys:\n",
    "            try:\n",
    "                d = d[key]\n",
    "            except (KeyError, IndexError, TypeError):\n",
    "                return default\n",
    "        return d\n",
    "\n",
    "    def split_date_time(datetime_str):\n",
    "        dt_obj = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))\n",
    "        date_str = dt_obj.date().isoformat()\n",
    "        time_str = dt_obj.time().isoformat()\n",
    "        return date_str, time_str\n",
    "\n",
    "    for fixture_data in data:\n",
    "        fixture = fixture_data['fixture']\n",
    "        venue = fixture['venue']\n",
    "        teams = fixture_data['teams']\n",
    "        goals = fixture_data['goals']\n",
    "        \n",
    "        fixture_id = fixture['id']\n",
    "        referee = fixture['referee']\n",
    "        datetime_str = fixture['date']\n",
    "        timestamp = fixture['timestamp']\n",
    "        status_short = fixture['status']['short']\n",
    "        venue_id = venue['id']\n",
    "        venue_name = venue['name']\n",
    "        venue_city = venue['city']\n",
    "        \n",
    "        home_team_id = teams['home']['id']\n",
    "        home_team_name = teams['home']['name']\n",
    "        home_winner = teams['home']['winner']\n",
    "        home_goals = goals['home']\n",
    "        \n",
    "        away_team_id = teams['away']['id']\n",
    "        away_team_name = teams['away']['name']\n",
    "        away_winner = teams['away']['winner']\n",
    "        away_goals = goals['away']\n",
    "        \n",
    "        date, time = split_date_time(datetime_str)\n",
    "        \n",
    "        extracted_data.append({\n",
    "            'fixture_id': fixture_id,\n",
    "            'referee': referee,\n",
    "            'date': date,\n",
    "            'time': time,\n",
    "            'timestamp': timestamp,\n",
    "            'status_short': status_short,\n",
    "            'venue_id': venue_id,\n",
    "            'venue_name': venue_name,\n",
    "            'venue_city': venue_city,\n",
    "            'home_team_id': home_team_id,\n",
    "            'home_team_name': home_team_name,\n",
    "            'home_winner': home_winner,\n",
    "            'home_goals': home_goals,\n",
    "            'away_team_id': away_team_id,\n",
    "            'away_team_name': away_team_name,\n",
    "            'away_winner': away_winner,\n",
    "            'away_goals': away_goals\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(extracted_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260866c",
   "metadata": {},
   "source": [
    "# myDataFetching6: STATISTICS_FIXTURES_ALL.JSON into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39964e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    start_time = time.time()\n",
    "    source_bucket_name = 'dwlprojectbucket'\n",
    "    target_bucket_name = 'datalakepartition2'\n",
    "    \n",
    "    # Only process statistics_fixtures_all.json files\n",
    "    pattern = 'statistics_fixtures_all.json'\n",
    "    \n",
    "    # Get the list of existing keys in the target bucket\n",
    "    existing_keys = get_existing_keys(target_bucket_name)\n",
    "    print(f\"Time to fetch existing keys: {time.time() - start_time} seconds\")\n",
    "    print(f\"Existing keys in target bucket: {existing_keys}\")\n",
    "    \n",
    "    # List all objects in the source bucket\n",
    "    response = s3.list_objects_v2(Bucket=source_bucket_name)\n",
    "    print(f\"Time to list objects in source bucket: {time.time() - start_time} seconds\")\n",
    "    if 'Contents' not in response:\n",
    "        print(\"No files found in source bucket.\")\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps('No files found in source bucket.')\n",
    "        }\n",
    "    \n",
    "    for obj in response.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        # Process only files that match the specified pattern and do not exist in the target bucket\n",
    "        if key.endswith(pattern):\n",
    "            # Extract the date and time from the filename and ensure it is formatted correctly\n",
    "            date_time = key.split(' - ')[0].strip()  # Adjust this split based on your actual filename pattern\n",
    "            output_file_key = f'{date_time} - {pattern.split(\".\")[0]}.csv'\n",
    "            if output_file_key not in existing_keys:\n",
    "                print(f\"Processing file: {key}\")\n",
    "                process_file(source_bucket_name, target_bucket_name, key, transform_statistics_fixtures, output_file_key)\n",
    "\n",
    "    print(f\"Total execution time: {time.time() - start_time} seconds\")\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('CSV files created successfully')\n",
    "    }\n",
    "\n",
    "def get_existing_keys(bucket_name):\n",
    "    existing_keys = set()\n",
    "    continuation_token = None\n",
    "    while True:\n",
    "        if continuation_token:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name, ContinuationToken=continuation_token)\n",
    "        else:\n",
    "            response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                existing_keys.add(obj['Key'])\n",
    "        if response.get('IsTruncated'):\n",
    "            continuation_token = response.get('NextContinuationToken')\n",
    "        else:\n",
    "            break\n",
    "    return existing_keys\n",
    "\n",
    "def process_file(source_bucket_name, target_bucket_name, key, transform_func, output_file_key):\n",
    "    try:\n",
    "        # Get the file from the source S3 bucket\n",
    "        fetch_start_time = time.time()\n",
    "        obj = s3.get_object(Bucket=source_bucket_name, Key=key)\n",
    "        print(f\"Time to fetch file {key}: {time.time() - fetch_start_time} seconds\")\n",
    "\n",
    "        read_start_time = time.time()\n",
    "        raw_data = obj['Body'].read().decode('utf-8').splitlines()\n",
    "        print(f\"Time to read and decode file {key}: {time.time() - read_start_time} seconds\")\n",
    "        \n",
    "        # Apply the transformation function\n",
    "        transform_start_time = time.time()\n",
    "        extracted_df = transform_func(raw_data)\n",
    "        print(f\"Time to transform file {key}: {time.time() - transform_start_time} seconds\")\n",
    "        \n",
    "        # Convert DataFrame to CSV\n",
    "        csv_buffer = StringIO()\n",
    "        csv_start_time = time.time()\n",
    "        extracted_df.to_csv(csv_buffer, index=False)\n",
    "        print(f\"Time to convert DataFrame to CSV for file {key}: {time.time() - csv_start_time} seconds\")\n",
    "        \n",
    "        # Upload the CSV to the target S3 bucket\n",
    "        upload_start_time = time.time()\n",
    "        s3.put_object(Bucket=target_bucket_name, Key=output_file_key, Body=csv_buffer.getvalue())\n",
    "        print(f\"Time to upload CSV for file {key}: {time.time() - upload_start_time} seconds\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {key}: {e}\")\n",
    "\n",
    "# Transformation function for statistics_fixtures_all.json\n",
    "\n",
    "def transform_statistics_fixtures(raw_data):\n",
    "    extracted_data = []\n",
    "    for line in raw_data:\n",
    "        try:\n",
    "            outer_list = json.loads(line)\n",
    "            for json_str in outer_list:\n",
    "                fixture = json.loads(json_str)\n",
    "                fixture_id = fixture['parameters']['fixture']\n",
    "                for fixture_data in fixture['response']:\n",
    "                    team = fixture_data['team']\n",
    "                    statistics = fixture_data['statistics']\n",
    "                    \n",
    "                    team_id = team['id']\n",
    "                    team_name = team['name']\n",
    "                    \n",
    "                    stats_dict = {stat['type']: stat['value'] for stat in statistics}\n",
    "                    \n",
    "                    extracted_data.append({\n",
    "                        'fixtureID': fixture_id,\n",
    "                        'team_Id': team_id,\n",
    "                        'team_name': team_name,\n",
    "                        'Shots on Goal': stats_dict.get('Shots on Goal'),\n",
    "                        'Shots off Goal': stats_dict.get('Shots off Goal'),\n",
    "                        'Total Shots': stats_dict.get('Total Shots'),\n",
    "                        'Blocked Shots': stats_dict.get('Blocked Shots'),\n",
    "                        'Shots insidebox': stats_dict.get('Shots insidebox'),\n",
    "                        'Shots outsidebox': stats_dict.get('Shots outsidebox'),\n",
    "                        'Fouls': stats_dict.get('Fouls'),\n",
    "                        'Corner Kicks': stats_dict.get('Corner Kicks'),\n",
    "                        'Offsides': stats_dict.get('Offsides'),\n",
    "                        'Ball Possession': stats_dict.get('Ball Possession'),\n",
    "                        'Yellow Cards': stats_dict.get('Yellow Cards'),\n",
    "                        'Red Cards': stats_dict.get('Red Cards'),\n",
    "                        'Goalkeeper Saves': stats_dict.get('Goalkeeper Saves'),\n",
    "                        'Total passes': stats_dict.get('Total passes'),\n",
    "                        'Passes accurate': stats_dict.get('Passes accurate'),\n",
    "                        'Passes %': stats_dict.get('Passes %'),\n",
    "                        'expected_goals': stats_dict.get('expected_goals')\n",
    "                    })\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(extracted_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee53517",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
